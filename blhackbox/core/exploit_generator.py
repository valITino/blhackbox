"""ExploitGenerator â€“ LLM-powered post-report exploit guidance.

This module is intentionally kept separate from the recon planner so
that the ethical constraints in the recon system prompt are never
weakened.  The exploit generator uses its own dedicated prompts with
strict guardrails (see ``llm/exploit_prompts.py``).
"""

from __future__ import annotations

import json
import logging
from typing import Any

from langchain_core.messages import HumanMessage, SystemMessage

from blhackbox.llm.client import get_llm
from blhackbox.llm.exploit_prompts import (
    SYSTEM_PROMPT_EXPLOIT,
    SYSTEM_PROMPT_EXPLOIT_NO_POC,
    build_exploit_user_prompt,
)
from blhackbox.models.base import Finding

logger = logging.getLogger("blhackbox.core.exploit_generator")


class ExploitGenerator:
    """Generate post-assessment exploit guidance from scan findings.

    By default, only high-level risk descriptions and remediations are
    produced.  When *allow_poc* is ``True`` the LLM is asked for
    descriptive proof-of-concept approaches (still not raw exploit code).
    """

    def __init__(self, *, allow_poc: bool = False) -> None:
        self._llm = get_llm()
        self._allow_poc = allow_poc

    async def generate(
        self,
        findings: list[Finding],
    ) -> list[dict[str, Any]]:
        """Ask the LLM for exploit guidance on the given findings.

        Returns a list of guidance dicts with keys: finding_id, title,
        exploitability, description, proof_of_concept, impact, remediation.
        """
        if not findings:
            return []

        # Only send findings with severity >= LOW (skip purely informational
        # noise like "port scan completed" to keep the prompt focused).
        actionable = [f for f in findings if f.severity != "info"]
        if not actionable:
            actionable = findings  # fall back to all if everything is info

        findings_payload = [
            {
                "id": f.id,
                "title": f.title,
                "severity": f.severity if isinstance(f.severity, str) else f.severity.value,
                "description": f.description[:2000],
                "evidence": f.evidence[:1000] if f.evidence else "",
                "tool": f.tool,
            }
            for f in actionable
        ]

        system_prompt = (
            SYSTEM_PROMPT_EXPLOIT if self._allow_poc else SYSTEM_PROMPT_EXPLOIT_NO_POC
        )
        user_prompt = build_exploit_user_prompt(
            json.dumps(findings_payload, indent=2),
            allow_poc=self._allow_poc,
        )

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ]

        logger.info(
            "Requesting exploit guidance for %d findings (poc=%s)",
            len(findings_payload),
            self._allow_poc,
        )
        response = await self._llm.ainvoke(messages)
        raw = response.content if hasattr(response, "content") else str(response)
        logger.debug("Exploit generator raw response length: %d", len(raw))

        return _parse_guidance_response(raw)


def _parse_guidance_response(raw: str) -> list[dict[str, Any]]:
    """Parse the LLM JSON array response, handling common formatting issues."""
    text = raw.strip()

    # Strip markdown code fences if present
    if text.startswith("```"):
        lines = text.split("\n")
        lines = [line for line in lines if not line.strip().startswith("```")]
        text = "\n".join(lines).strip()

    try:
        data = json.loads(text)
    except json.JSONDecodeError:
        # Try to extract a JSON array from the response
        start = text.find("[")
        end = text.rfind("]") + 1
        if start >= 0 and end > start:
            try:
                data = json.loads(text[start:end])
            except json.JSONDecodeError:
                logger.warning("Could not parse exploit guidance as JSON")
                return []
        else:
            logger.warning("No JSON array found in exploit guidance response")
            return []

    if not isinstance(data, list):
        logger.warning("Exploit guidance response is not a JSON array")
        return []

    # Validate each entry has required keys
    required_keys = {"finding_id", "title", "exploitability", "description", "remediation"}
    validated: list[dict[str, Any]] = []
    for entry in data:
        if not isinstance(entry, dict):
            continue
        if required_keys.issubset(entry.keys()):
            validated.append(entry)
        else:
            logger.debug("Skipping malformed guidance entry: %s", list(entry.keys()))

    return validated
